%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EL Advance Electronic Engineering Report
% University of Southampton
%
% author : Lawrence Gray (lg5g15)
%
% edited : 2016-11-14
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,11pt]{article}
\setlength\parindent{0pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PACKAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\usepackage{graphicx}
\usepackage[margin=2mm]{caption}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{cancel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT BEGIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
  
\begin{center}

{\Large{\textbf{Digital Control ELEC3206:}}} \\ 
{\Large{\textbf{Lecture notes by Lawrence Gray}}} \\ [\baselineskip]
\end{center}
\section{Lecture 1: Intro }
	This course concerns using digital control using impulse sampling. The data is sampled using a train of impulses, in this example $x(t)$ is being sampled:
\begin{equation}
	x^*(t):= \sum\limits_{k=0}^{\infty}x(kT)\delta(t-kT)
	\end{equation}
	\subsection{Laplace to Z-Transform}
		The Laplacian transformation of a function $f(t)$ is:\\
		\begin{equation}
		\mathcal{L}(f):= \int\limits_{0}^{\infty}e^{-st}dt
		\end{equation}

		Example:\\
		
		\[ f(x) =\begin{cases} 
      			0 & t < 0\\
      			e^{\alpha t}& t \geq 0
   			\end{cases}
			\]

		\begin{equation}
		\mathcal{L}(f(t)):= \int\limits_{0}^{\infty}e^{\alpha t}e^{-st}dt =\int\limits_{0}^{\infty}e^{(\alpha -s)t}dt= \frac{1}{s-\alpha}		\end{equation}
		Below are some examples using the impulse response which is used a lot in this course:
		\begin{equation}
		    \mathcal{L}(\delta(\cdot))=1
		    \qquad
		    \mathcal{L}(\delta(\cdot-\tau))=e^{-s \tau}
		\end{equation}
		We can then simplify this further than Laplace as we will always will be using a impulse.
		\begin{equation}
		    X^*(s)=\sum\limits_{k=0}^{\infty} x(kT)\mathcal{L}(\delta(\cdot-kT)) = \sum\limits_{k=0}^{\infty}x(kT)e^{-skT}= \sum\limits_{k=0}^{\infty}x(kT)z^{-k}
		\end{equation}
		We have defined z as:
		\begin{equation}
		    z=e^{sT}
		\end{equation}
		This brings the following definition of the Z transform:
		\begin{equation}
		    X(z):= \sum\limits_{\infty}^{k=0}x(kT)z^{-k}	
		\end{equation}		
\newpage
	\subsection{Z-transform of a sequence}
	For a sequence $\{x(kT)\}_{k=0,1...}$ we can find the Z transform of some common sequences:\\
	Sequence 1 is a impulse at 0 (Discrete Impulse):
	
		\[ x(kT) =\begin{cases} 
      			0 & k < 0\\
      			1 & k = 0\\
			0 & k > 0   			
   			\end{cases}
			\]
		Z-transform:
		\begin{equation}
		    X(z)= \sum\limits_{\infty}^{k=0}x(kT)z^{-k}=1\cdot z^{-0} = 1
		\end{equation}
	Sequence 2:
			\[ x(kT) =\begin{cases} 
      			0 & k < 0\\
      			1 & k = 0,1\\
			0 & k > 1   			
   			\end{cases}
			\]
		Z-transform:
		\begin{equation}
		    X(z)= \sum\limits_{\infty}^{k=0}x(kT)z^{-k}=1\cdot z^{-0} +1\cdot z^{-1}= 1+\frac{1}{z} = \frac{z+1}{z}
		\end{equation}
	Sequence 3:
			\[ e^{\alpha \cdot}  =\begin{cases} 
      			0 & t < 0\\
      			e^{\alpha t} & t \geq 0			
   			\end{cases}
			\]
		The laplace transform of this is given to be:
		\begin{equation}
		   \mathcal{L}(e^{\alpha \cdot})= \frac{1}{s-\alpha}
		\end{equation}
		To get the z transform of this we ``sample'' the signal  by stating $t=kT$ which makes our input function $e^{\alpha kT}$.\\
		So what is the Z-transform of $\{(e^{\alpha T})^{k}\}_{k=0,1...}$
		\begin{equation}
		    X(z)= \sum\limits_{\infty}^{k=0}(e^{\alpha T})^{k}z^{-k}=\sum\limits_{\infty}^{k=0}(\frac{e^{\alpha T}}{z})^{k}
		\end{equation}
		Here we assume that z is such that $|\frac{e^{\alpha T}}{z}|<1$. This then means that we can simply use the geometric series.
		\begin{equation}
		    X(z)= \sum\limits_{\infty}^{k=0}(\frac{e^{\alpha T}}{z})^{k}=\frac{1}{1-\frac{e^{\alpha T}}{z}}=\frac{z}{z-e^{\alpha T}}
		\end{equation}
	\subsection{Z-Transform rules}
		With a Z transform we convert a set of series of real numbers to a set of formal power series.
	Some Simple rules of the Z-transform (same as Laplace I think):
		\begin{equation}
		    \mathcal{Z}(\{x_{k}+y_{k}\}) = \mathcal{Z}(\{x_{k})+\mathcal{Z}(y_{k}\})
		\end{equation}
		\begin{equation}
		    \mathcal{Z}(\{\alpha x_{k}\}) = \alpha\mathcal{Z}(\{x_{k}\})
		\end{equation}
	The Left Shift rule, here we are deriving the the Z-transform of $\mathcal{Z}(x_{k+1})$ we use a simple table to do so:
	

	\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
	 \hline
	 k & 0 & 1 &2&3\\ 
	 \hline
	 $\{x_{k}\}$& $x_{0}$ & $x_{1}$& $x_{2}$& $x_{3}$\\ 
	\hline	 
	 $\{x_{k+1}\}$& $x_{1}$& $x_{2}$& $x_{3}$& $x_{4}$\\ 
	 \hline
	\end{tabular}
	\end{center}
	\begin{equation}
		X(z):= \sum\limits_{\infty}^{k=0}x_{k+1}z^{-k}=	x_{1} + x_{2}z^{-1} +x_{3}z^{-2}...= z\mathcal{Z}(\{x_k\})-zx_0
	\end{equation}
\section{Lecture 2:}
	Right Shift Rule $\{x(k-1)\}$ is simpler than the left shift rule:
	\begin{equation}
		\mathcal{Z}(\{x(k-1)\})=z^{-1}\mathcal{Z}(\{x(k)\})
	\end{equation}
	Convolution, $\{h(k)\}:=\{x(k)\}*\{y(k)\}$, is also a simple rule:
	\begin{equation}
		h(k) = \sum\limits_{k}^{j=0}y_{k-j}x_j
	\end{equation}
	\begin{equation}
		\mathcal{Z}(\{h(k)\})=\mathcal{Z}(\{x(k)\})\mathcal{Z}(\{y(k)\})
	\end{equation}\\
	Initial Value theorem, for our initial value $x_0$:
	\begin{equation}
		\mathcal{Z}(\{x(k)\})=x_0+x_1x^{-1}...
	\end{equation}
	From this we can then find the initial value as z tends to $\infty$ 
	\begin{equation}
		x_0=\lim\limits_{z \rightarrow \infty}\mathcal{Z}(\{x_k\})
	\end{equation}
	\subsection {Final Value Theorem:}
		if $\mathcal{Z}(\{x_k\})$ has all of it's singularities in the open unit circle except possibly in the open unit circle except possibly one at $z=1$ then $\lim\limits_{Z\Rightarrow\infty}(x_k)=\lim\limits_{z\Rightarrow\infty}(1-z^{-1})\mathcal{Z}(\{x_k\})$.\\
		Note, for this course, almost all Z-transforms are rational functions so ``singularity" means 0 at the denominator/``pole"
	\subsection{Fibonacci sequence/Difference equation}
		A Fibonacci sequence is defined as:
		\begin{equation}
		    y(k+2)=y(k+1)+y(k)
		\end{equation}
		But in digital control we tend to see this with an extra term which we think of as our input:
		\begin{equation}
		    y(k+2)-y(k+1)-y(k) = u(k)
		\end{equation}
		The main features of the difference equation are:
		\begin{itemize}
		    \item Order = highest shift - lowest shift
		    \item Linear
		    \item coefficients do not depend on time 
		\end{itemize}
		A real world example is a bank account, where $S(k)$ is the sum of money, $\alpha(k)$ is the interest rate and $u(k)$ represents deposits and withdrawals:
		\begin{equation} 
			S(k+1)= S(k) +\alpha(k)S(k) +u(k)
		\end{equation}
	\subsection{Difference equation in Signal processing}
		\begin{equation} 
			y(k+2)-y(k+1)-y(k)=u(k)
		\end{equation}	
		\begin{equation} 
			\mathcal{Z}(y(k+2))-\mathcal{Z}(y(k+1))-\mathcal{Z}(y(k))=\mathcal{Z}(u(k))
		\end{equation}
		We will derive the second shift to preform a Z-transform on the above equation.\\
		The second shift, $y(k+2)$ is the left shift of $y(k+1)$
		\begin{equation} 
			\mathcal{Z}(y(k+2))=z\mathcal{Z}(y(k+1))-zy_1=z(zY(z)-zy_0)-zy_1
		\end{equation}
		Now we can derive $Y(z)$:
		\begin{equation} 
			(z^2Y(z)-z^2y_0-zy_1)-(zY(z)-zy_0) -Y(z) = U(z)
		\end{equation}	
		\begin{equation} 
			(z^2-z-1)Y(z) = z^2y_0+zy_1-zy_0 +U(z)
		\end{equation}
		\begin{equation} 
			Y(z) =\frac{z^2y_0+zy_1-zy_0}{z^2-z-1}+\frac{1}{z^2-z-1}U(z)
		\end{equation}	\\
		FREE RESPONSE: The first expression on the right hand side is the Z-transform of the solution to the equation $y(k+2)-y(k+1)-y(k)=0$ with initial conditions $y_0$ and $y_1$. This is how the system acts if no external (force) is applied.\\\\
		FORCED RESPONSE: The second expression of the right hand side is the Z-transform of the solution to the equation $y(k+2)-y(k+1)-y(k)=0$ with initial conditions $y_0=y_1=0$ and the "forcing term" (input) being $u(k)$.\\
		$\mathcal{Z}("sequence")=Transfer Function$
\section{Lecture 3: Stability}
	Issue: how to characterise the system s that (for zero initial conditions) react reasonably to a reasonable input.\\
	We define reasonable as ``bounded". $\{u_k\}$ is bounded if there exists an $M$ such that $|u_k|<M $ for $k=0,1,...$\\
	A system is said to be stable if bounded inputs produce bounded outputs (for zero initial conditions).\\\\
	For this next section we will be considering the equation $Y(z)=H(z)U(z)$ where H(z) is the transfer function.\\
	$U(z)$ rational function: when is it the Z-transform of a bounded signal?\\
	for $U(k)=\{\alpha^k\} \rightarrow^\mathcal{Z} \frac{z}{z-\alpha}$ the condition $|\alpha|<1$(within unit circle) must hold true to be bounded.\\
	$Y(z)$ rational comes from a bounded sequence if and only if all of its poles are in an open unit disk ($\mathcal{D}$).\\
	From this, if $U(z)$ has all of its poles in $\mathcal{D}$ then $Y(z)$ has all its poles in $(\mathcal{D})$ of and only if $H(z)$ has all of its poles in the unit circle $\mathcal{D}$.\\\\
	Theory:
	\indent The system with TF $H(z)$ is bounded input bounded output stable (BIBO-stable) if and only if all the poles of $H(z)$ are in $\mathcal{D}$.\\
	Note the "open" unit dis is for $|z|<1$ not \sout{$|z|\leq1$}.\\
	Example of unbounded:
	\begin{equation}
		H(z) = \frac{z}{z-1} \qquad \{u_k\}=\{1,1,1,1...\}
	\end{equation}
	\begin{equation}
	    	Y(z) = H(z)U(z)=\frac{z}{z-1}\frac{z}{z+1}\Rightarrow\{0,1,2,3...\}
	\end{equation}
	Therefore this is unbounded.\\
	\subsection{Sampling}
		\textbf{DAC:(zero order hold)}\\
		The zero order hold keeps the sample constant between samples. 
		\begin{equation}
			U(t)=U_d(k)\qquad kT\leq t < (k+1)T		
		\end{equation}
		\textbf{Sample and Hold block model:}\\
		To understand this we will use a unit step input defined as:
		\[\bar{1}(t) =\begin{cases} 
      			1 & t \geq 0\\
      			0 & t < 0			
   			\end{cases}
			\]
			
		\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.4]{images/sample_and_hold}
		\end{figure}
		\begin{equation}
			u'(t)=\sum\limits_{k=0}^{\infty}u(kT)[\bar(1)(t-kT)-\bar(1)(t-(k+1)T)]		
		\end{equation}
		As I cant be bothered to draw it i'm going to describe what this is doing. This part $[\bar(1)(t-kT)-\bar(1)(t-(k+1)T)]$ creates a unit pulse of length T between $kT$ and $(k+1)T$.\\
		We next take the laplace of that: 
		\begin{equation}
			\mathcal{L}(u')=\sum\limits_{k=0}^{\infty}u(kT)[\mathcal{L}(\bar(1)(t-kT))-\mathcal{L}(\bar(1)(t-(k+1)T))]
		\end{equation}	
		\begin{equation}
			=\sum\limits_{k=0}^{\infty}u(kT)[\frac{e^{-(k+1)Ts}}{s}-\frac{e^{-kTs}}{s}]=\sum\limits_{k=0}^{\infty}u(kT)[\frac{e^{-kTs}(1-e^{-Ts})}{s}]
		\end{equation}
		\begin{equation}
			\mathcal{L}(u')=\frac{1-e^{-Ts}}{s}U^*(s)
		\end{equation}	
		\subsection{Frequency response of ZOH}
		\begin{equation}
			G(s)=\frac{1-e^{-Ts}}{s}\rightarrow G(jw) = 1-e^{-Tjw}=2e^{-Tjw}\frac{e^{\frac{T}{2}jw}-e^{\frac{-T}{2}jw}}{2jw}
		\end{equation}
		\begin{equation}
			=\frac{T\sin(w\frac{T}{2})}{w^{\frac{T}{2}}}e^{-\frac{T}{2}jw}
		\end{equation}
		where $\frac{T\sin(w\frac{T}{2})}{w^{\frac{T}{2}}}$ is the sinc function.\\
		\FloatBarrier
		\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.4]{images/sinc}
		\end{figure}
		
		I wrote this down but cannot remember the relevance any more.\\
		\begin{equation}
		    	(g*u)(t)=\int\limits_{-\infty}^{\infty}g(t-\tau)u(\tau)d\tau
\end{equation} 
\section{Lecture 4: Reconstruction of signals from samples}
\subsection{First Order Sample and hold}
	The for the input $u(t)$, sampled value of $\{u(kT)\}$ and the output $u'(t)$.
	The First order hold is defined as:
	\begin{equation}
		u'(t):= u_d(kT)\frac{t+T}{T}-u_d((k-1)T)\frac{t}{T} for kT \leq t < (k+1)T
	\end{equation}
	When we take $u_d$ as the step input ($\bar{1}$) we get:
	\begin{equation}
		u'(t)= \bar{1}(t)\frac{t+T}{T}-\bar{1}(t-T)\frac{t-T}{T}-\bar{1}(t-T) 
	\end{equation}
	Using the laplace and the shifting rules we obtain:
	\begin{equation}
		\mathcal{L}(u')= (\frac{1}{Ts^2}+\frac{1}{s})-e^{-Ts}\frac{1}{Ts^2}-e^{-Ts}\frac{1}{s1}= (1-e^{-Ts})\frac{Ts+1}{Ts^2}
	\end{equation}	

\subsection{Reconstruction of signals from samples}
	Shannon's sampling theorem:\\
	For a signal $f(.)$ which is band limited over the whole real axis, therefore $F(w)=0$ for $w<-w1$ and $w>w_1$.(This is basically just stating that that the maximum frequency is given as $w_1$).\\
	Shannon's theorem state that, if the sampling frequency $ w_s > 2 w_1$ then $f(.)$ can be uniquely reconstructed from its samples.\\
	Moreover...what is the significance of this?
	\begin{equation}
		f(t)=\sum\limits_{\infty}^{k=-\infty}f(kT)\frac{sin(\frac{w_s(t-kT)}{2})}{\frac{w_s(t-kT)}{2}}= \sum\limits_{\infty}^{k=-\infty}sinc(\frac{w_s(t-kT)}{2})
	\end{equation}
	
	Proof of this can be done using Fourier series. Here is a quick definition of how to move from frequency domain to the time domain:
	\begin{equation}
		F(w)=\int\limits_{-\infty}^{\infty}e^{-jwt}f(t)dt
	    	\qquad
	    	f(t)=\frac{1}{2\pi}\int\limits_{-\infty}^{\infty}e^{jwt}F(w)dw
	\end{equation}	
	We then define the fourier transform of the sampled signal as:
	\begin{equation}
		F_s(w):=\frac{1}{T}\sum\limits_{k=-\infty}^{\infty}F(w+kw_s)
	\end{equation}	
	If $w_s>2w_1$:
				\[ F(w) =\begin{cases} 
      			TF_s(2) & |w|\leq \frac{w_s}{2}\\
      			0 & |w| >\frac{w_s}{2} 			
   			\end{cases}
			\]
	
	Fourier series of $F_s$ is: 
	\begin{equation}
	    F_s(w) = \sum\limits_{k=-\infty}^{\infty}C_ke^{-jwkT}
	\end{equation}
	where: 
	 \begin{equation}
	    C_k=\frac{1}{w_s} \int\limits_{0}^{\infty}e^{jwkT}F_s(w)dw
	\end{equation}
	
	
	Compute:
	 \begin{equation}
	    f(kT)=\frac{1}{2\pi}\int\limits_{-\infty}^{\infty}e^{jwkT}F(w)dw=\frac{1}{2\pi}\int\limits_{-\frac{w_s}{2}}^{\frac{w_s}{2}}e^{jwkT}TF_s(w)dw=\frac{1}{w_s}\int\limits_{-\frac{w_s}{2}}^{\frac{w_s}{2}}e^{jwkT}F(w)dw= C_k
	\end{equation}
		
	Apparently this shows that we can reconstruct $F_s(w)$.\\
	
	Causal definition: "A system is said to be causal system if its output depends on present and past inputs only and not on future inputs."\\
	The sinc function is not causal, even approximating it by truncation (waiting for more inputs before making decision), a delay is introduced. Not suitable for time critical applications in control.\\
	
	\subsection{Aliasing \& Folding}
	The forrier series of the sampled signal was previously given as:
	 \begin{equation}
		F_s(w):=\frac{1}{T}\sum\limits_{k=-\infty}^{\infty}F(w+kw_s)
	\end{equation}
	Here you should consider what this is representing. Imagine $F(w)$ is the frequency response which will be centred around 0. Therefore the summation $F(w+kw_s)$ creates many copies of that given response. For $w_s>2w_1$ we can see that the spectra do not intersect (in this case then signal can be reconstructed).
	\FloatBarrier
	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.5]{images/aliasing_and_folding_1}
	\end{figure}
	But if we have $w_s\leq0$ then the spectra intersect. This is also known as the folding phenomenon. 
	
	\FloatBarrier
	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.25]{images/aliasing_and_folding_2}
	\end{figure}
	Summary: 
	"If $\frac{w_s}{2}\leq$ then frequencies higher than appear “folded” between 0 and $\frac{w_s}{2}$. Sampling at frequencies lower than the Nyquist one $\frac{w_s}{2}$ can hide frequencies."
	\subsection{Example: Sinusoidal input to sample-and-hold}
	I would take this with a pinch of salt, the maths are all valid but i'm not sure what this achieves really.\\
	modulate $u(t)= sin(wt+\phi)=im(e^{j(wt+\phi)})$ with $\sum\limits_{k=-\infty}^{\infty}\delta(t-kT)$ which has the representation:
	\begin{equation}
		\sum\limits_{k=-\infty}^{\infty}e^{-kjw_st}= 2\sum\limits_{k=0}^{\infty}cos(kw_st)    
	\end{equation}
	Here is a quick proof of this as given in the lecture:
	\begin{equation}
		2\sum\limits_{k=0}^{\infty}cos(kw_st)= ...+e^{2jw_st}+e^{jw_st}1+1+e^{-jw_st}+ e^{-2jw_st}+...    
	\end{equation}
	Using the exponential definition of cos(t):
	\begin{equation}
		cos(t)= \frac{e^{jt}+e^{-jt}}{2} 
	\end{equation}
	You can then pair up the terms and see that the previous equation holds true.\\
	End of proof.\\\\
	
	The output of the sampler is:
	\begin{equation}
		\frac{1}{T}(\sin(wt+\phi)+2\sum\limits_{k=1}^{\infty}\cos(kw_st)\sin(wt+\phi) 
	\end{equation}
	which can be found to be equal to:
	\begin{equation}
		\frac{1}{T}(\sin(wt+\phi)+2\sum\limits_{k=1}^{\infty}\sin(kw_st+wt+\phi) -\sin(kw_st-wt-\phi))
	\end{equation}
	This finds that there are frequency components at $w\pm kw_s$:
	\FloatBarrier
	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.5]{images/sample_and_hold_freq}
	\end{figure}
	Through zero order hold block, for $w\neq\frac{w_s}{2}$, fundamental component at $w$ is transformed to:
	\begin{eqnarray}
		u_r(t)=\frac{1}{T}im(G(jw)e^{j(wt+\phi)})
	\end{eqnarray}
	But for $w=\frac{w_s}{2}$ this produces two components in the sampled signal. in this case the output after a zero order hold bloc:
	\begin{eqnarray}
		u_r(t)=\frac{1}{T}im(2e^{j(\pi -\phi)}sin(\phi)G(jw)e^{j(wt+\phi)})
	\end{eqnarray}
	Notice the dependence on $\phi$ as in this case if it is equal to $\pi$ then $u_r=0$.\\
	Sampling at or under the Nyquist rate for reconstruction the phase is crucial therefore you should never trust the reconstructed signal.\\
	
	In his slides he then gives graphical examples of this, but its quite self explanatory, if the samples occur whenever the signal is 0 then you have nothing to reconstruct!.
	
	
	\subsection{Choice of sampling period}
	\begin{itemize}   
		\item Choice of T related to physical, technological, cost, etc. factors
		\item Shannon's result = ideal situation: infinite accuracy, band limited signal, perfect reconstruction.
		\item In reality: quantization effect, un-modelled dynamics, disturbances, structure of controller, unknown highest frequency $w_1$, etc.
		\item Effect: spurious low frequencies due to folding and aliasing of high frequencies
		\item Rule of thumb: take frequency $2w_1$, and multiply by 5 to 20 in order to find sampling frequency.
		\item Too high? Then decrease and check if controlled system still performs well.
		\item Or, use anti-aliasing filters (Bessel, Butterworth, etc.) to band-limit signal before sampling in order to obtain reasonable sampling rate.
	\end{itemize}
	
	\subsection{Pre filtering} 
	This is quite self explanatory so I'm just going to copy from the slides.\\	To avoid aliasing problems, it may be useful to
pre-filter the analogue signals before sampling.\\
	Analogue sensors have some filter integrated, but it is
not especially designed for your control problem!\\
Easier way out: introduce analogue filter (e.g. Bessel)
before the sampler. That will reduce the input signal
bandwidth, and guarantee that no aliasing appears.\\
If the analogue filter has non-negligible effects on the
dynamics of the control system, then it must be taken
into account in the control design.\\
	\section{Lecture 5: Pulse Transfer Functions} 
	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.6]{images/PTF}
	\end{figure}
	\FloatBarrier
	
	``The transfer function is a faithful representation of the I/O behaviour of a linear system at rest."
	With the transfer function being $H(s)$: 
	\begin{equation}
		Y(s)=H(s)U(s)
	\end{equation}
	If I do not know $H(s)$, in principle I can recover it from experiments.
	\begin{equation}
		H(s):=\frac{Y(s)}{U(s)}
	\end{equation}
	The procedure to find the PTF is:
	\begin{enumerate}[a)]
		\item take $u(.)$ (discrete time) and take its Z-transform $U(z)$
		\item ZOH $u(.)$ and find $y(.)$ at continuous time at ($y_c$) 
		\item (Ideally) sample $y(.)$ and Z-transform the result by obtaining $Y(z)$
		\item Define PTF by $\frac{Y(z)}{U(z)}$
	\end{enumerate}
	In principle, ant $u(.)$(Discrete time) could do for the experiment but life is easier if $u_k=1$ for $k=0,1,...$\\
	The reason for this is that the ZOH reconstruction of a discrete time step is equivalent to a continuous time step.
	The relation between $y_c(.)$ and $u_c(.)$ is(in the Laplace domain):
	\begin{equation}
		Y_c(s)=\frac{1}{s+\alpha}U_c(s)
	\end{equation}  
	Since ${u_k}={1,1,1...}$ $\rightarrow$ $\bar1(.)$(heaviside step) then $\mathcal{L}(u_c)=\frac{1}{s}$\\\\
	Therefore we can find:
	\begin{equation}
		Y_c(s)=\frac{1}{s+\alpha}\frac{1}{s}=\frac{-\frac{1}{\alpha}}		{s+\alpha}+\frac{\frac{1}{\alpha}}{s}
	\end{equation}
	\begin{equation}
		y_c = -\frac{1}{\alpha}(e^{-\alpha t})+\frac{1}{\alpha}\bar1(t) \qquad t\geq 0
	\end{equation}
	Sampling with sample period T and take Z transform:
	\begin{equation}
		\mathcal{Z}(y_k)=-\frac{1}{\alpha}\mathcal{Z}(e^{\alpha kT})+\frac{1}{\alpha}\mathcal{Z}(1)=-\frac{1}{\alpha}\frac{z}{z-e^{-\alpha T}}+\frac{1}{\alpha}\frac{z}{z-1}
	\end{equation}
	From this we can define $Y(z)$:
	\begin{equation}
		Y(z) := \frac{z(1-e^{1-\alpha T})}{\alpha (z-e^{-\alpha T})(z-1)}
	\end{equation}			
	We have already discussed the definition of $U(z)$:
	\begin{equation}
		U(z) := \mathcal{Z}(\{u_k\})=\frac{z}{z-1}
	\end{equation}	
	We can now get $H(z)$(PTF) from these two above equations:
	\begin{equation}
		H(z)=\frac{Y(z)}{U(z)}=\frac{1-e^{-\alpha T}}{\alpha(z-e^{-\alpha T})}
	\end{equation}
	He then drew a table kind collating the information obtained above but I don't want to draw that out now.\\
	In general we try to conciser an $\alpha$ value less than 0.\\
	
	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.4]{images/CT-DT}
	\end{figure}
	Some general points about PFT:
	\begin{enumerate}[a)]
		\item Discretion via PFT involves scaling
		\item The PFT may have zeros even if the (CT)TF did not have any.
		\item A CT (stable/unstable) TF is made discrete into a DT (stable/unstable) TF (because poles in the LHP are transformed into/outside of the unit disk)\\
	\end{enumerate}
	A ``Zero" is by definition a root of the numerator of the TF, but we need to think about its dynamical interpretation. Lets consider:
	\begin{equation}
		H(z) = \frac{z-4}{(z-2)(z-3)}\qquad U(z)= \frac{z}{z-\alpha}\qquad Y(z)=\frac{z-4}{(z-2)(z-3)}\frac{z}{z-\alpha}
	\end{equation} 
	\begin{equation}
		=\frac{A}{z-2}+\frac{B}{z-3}+\frac{C}{z-\alpha}
	\end{equation} 
	when does $C=0$? If and only if $\alpha = 4$\\
	``Zeros represent frequencies which ``disappear" when provided as an input to the system"\\
	Mapping poles from s space to z space can be seen here:
	\FloatBarrier
	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.3]{images/mapping}
	\end{figure}
	\FloatBarrier
	
	note on cascaded element, in the below figure the top sequence is not equal to the bottom. 
	\FloatBarrier
	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.7]{images/cascading}
	\end{figure}
	\section{Lecture 6: State-Space Equations}
	In order to give an introduction to state space equations we will be giving an example of a simple one first.
	If we consider the population of a species at time k.\\
	$Y(k)$ is the number of young individuals at time k.\\
	$M(k)$ is the number of mature individuals at time k.\\
	$O(k)$ is the number of old individuals at time k.\\
	This then leads to a set of equations which determines these three things.\\	
	$O(k+1)=\alpha M(k)$ for $0 \leq \alpha \leq 1$ (you need to be mature before you can be old)\\
	$M(k+1)=\beta Y(k)$ for $0 \leq \alpha \leq 1$ (you need to be young before you can be mature)\\
	$Y(k+1)=\gamma M(k)$ for $0 \leq \alpha \leq 1$ (mature people give birth to kids)\\\
	we can then arrange this into matrix form:
	
\begin{gather}
x(K+1)=
 \begin{bmatrix} Y(k+1) \\ M(k+1 )\\ O(k+1)\end{bmatrix}
 =
  \begin{bmatrix}
	0 & \gamma & 0 \\
	\beta & 0 & 0 \\
	0 & \alpha & 0 
   \end{bmatrix}
    \begin{bmatrix} Y(k) \\ M(k)\\ O(k)\end{bmatrix}
    := A x(k)
\end{gather}

Using this we will consider a close population system ($x(0)$ is known), therefore we can start deriving the population at time k+1:\\
$k=0$ $x(1)=Ax(0)$\\
$k=1$ $Ax(1)=AAx(0)=A^2x(0)$ and so on...\\
which brings about the formula $x(k)=A^kx(0)$\\\\
Now we consider more complications in our model (Migration/Culling/Immigration):\\
To do this we consider $u(k)$ which is the number of population entering/exiting at time k.\\
\begin{gather}
 \begin{bmatrix} \eta \\ \delta \\ \epsilon\end{bmatrix}u(k)
\end{gather}
where $\eta$ is the number of young entering/exiting ... and so on. Also note $\eta+\delta+\epsilon = 1$. Adding this to our original equation gets us:
\begin{gather}
x(k+1)=
 \begin{bmatrix} Y(k+1) \\ M(k+1 )\\ O(k+1)\end{bmatrix}
 =
  \begin{bmatrix}
	0 & \gamma & 0 \\
	\beta & 0 & 0 \\
	0 & \alpha & 0 
   \end{bmatrix}
    \begin{bmatrix} Y(k) \\ M(k)\\ O(k)\end{bmatrix}
\end{gather}
We can also calculate the total populations using:
\begin{gather}
y(k)=
 \begin{bmatrix} 1\\ 1\\ 1\end{bmatrix}
	x(k)
\end{gather}

this leads the the following two equations:
\begin{equation}
	x(k+1) = A x(k) + Bu(k)
\end{equation}
\begin{equation}
	y(k) = C x(k) + Du(k)
\end{equation}

Some important information about these is that $x(k)$ has n components, $u(k)$ has m components and $y(k)$ has p components. Which leads to $A\in \mathbb{R}^{n\times n}$, $B\in \mathbb{R}^{n\times m}$, $C\in \mathbb{R}^{p\times n}$ and  $D\in \mathbb{R}^{p\times m}$.\\\\
Possible equations for the population model:
\begin{enumerate}[a)]
	\item Which policy (choice of $\eta$, $\delta$ and $\epsilon$) of culling/introduction guarantees that a specific distribution in young-mature-old is maintained? ``controllability"
	\item How to compute or determine distribution young-old-mature looking at $y(.)$? ``observability"
	\item Will the population die out or will it reach stable value or will it explode ? ``stability"
	\item Maximise a quadratic function of the state space equation and the input. ``optimal control"
\end{enumerate}

Solution to the state-space equation:\\
$x(0)$ given $x(k+1)= Ax(k)+Bu(k)$ where u(0), u(1)... is given.\\
For discrete time:
\begin{equation}
	x(k) = A^kx(0)+\sum\limits_{k-1}^{j=0}A^{k-1-j}u(j)    
\end{equation}
for continuous time:
\begin{equation}
	x(t) = e^{A\tau}+\int\limits_{0}^{t}e^{A(t-\tau)}Bu(\tau)d\tau    
\end{equation}
\section{Lecture 7 ( State-Space Self Study)}
This worksheet contains 2 methods of solving state-space equations, both in the time domain and the Z-domain. (basic idea: Z domain is easier than time domain). I will skip over the solution in the time domain for now as it seems this was just trying to prove a point.
\subsection{Z-Transform solution}
We will consider the state-space equation from the previous lecture. But first we will rewrite $y(k)$ in terms of $x(0)$:
\begin{equation}
    y(k) = CA^kx(0)+\sum\limits_{k-1}^{j=0}CA^{k-1-j}Bu(j) + Du(k)  
\end{equation}
Using Z-Transforms make sure you are happy deriving the following from the definition of the $x(\cdot)$:
\begin{gather}
	zX(z) - zx(0) = \mathcal{Z}(Ax(.)+Bu(.))= AX(z)+BX(z)\\
	\qquad \rightarrow zX(z)-AX(z)=zx(0)+BU(z)\\
	\qquad \rightarrow (zI - A)X(z)= zx(0)+BU(z)
\end{gather}
From here it is quite simple. You can see that in order to find $X(z)$ you must find $(zI-A)^{-1}$ and then just simplify the right hand size. From this take the inverse transform to get x(k).
\subsection{The rest}
	There was a bit more to this lecture but as it was mainly an exercise it's kind of hard to write in note form. But here are some other notable things that should be taken, mainly concerning matrix multiplication.\\\\
	A matrix $A$ can be written as a combination of matrices, this is known as spectral decomposition. An example of this is $A=T\Lambda T^{-1}$ where $\Lambda$ is a diagonal matrix of eigenvalues of A and T is a matrix of the eigenvectors corresponding to those eigenvalues. This is a very useful method of breaking down a matrix as we will see here: 

	\begin{gather}
		A^2=(T\Lambda T^{-1})(T\Lambda T^{-1})=T\Lambda (TT^{-1})\Lambda T^{-1}=T\Lambda \mathbb{I}\Lambda T^{-1} = T\Lambda^2T^{-1}
	\end{gather}
	This then follows to a useful rule of:
	\begin{gather}
		A^k=T\Lambda^kT^{-1}
	\end{gather}
	This makes calculations much easier as you are taking the square of a diagonal matrix which is simply just squaring each element.\\\\
	There may be other things that appeared in this lecture which I have skimmed over but they should be covered in lecture 8 anyway.
\section{Lecture 8: The three hours.}
\subsection{Pulse Transfer Function(PTF)}
	Is there an analogous of the PTF for state space representations?\\
	To start with we will have an example of the state space equation in continuous time:
	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.3]{images/force}
	\end{figure}
	\FloatBarrier
	We can use this equation to describe the above system, note that $q$ is the distance from the wall:
	\begin{equation}
		m\frac{d^2q}{dt^2} + kq + b\frac{dq}{dt} = F
	\end{equation}
	Rearrange for $\frac{d^2q}{dt^2}$:
	\begin{equation}
		\frac{d^2q}{dt^2}= \frac{F}{m}- \frac{k}{m}q - \frac{b}{m}\frac{dq}{dt}
	\end{equation}
	Then we define the variable x as:
	\begin{gather}
		x_c:=
 		\begin{bmatrix} q \\ \frac{dq}{dt}\end{bmatrix}
	\end{gather}
Therefore we can then write:
\begin{gather}
	x_c=
	\begin{bmatrix} \frac{dq}{dt} \\ \frac{d^2q}{dt^2}\end{bmatrix}
 =
  \begin{bmatrix}
	0 & 1 \\
	-\frac{k}{m}& -\frac{k}{b}
   \end{bmatrix}
  \begin{bmatrix} q \\ \frac{dq}{dt}\end{bmatrix} 
  +\begin{bmatrix} 0 \\ \frac{1}{m}\end{bmatrix}F
\end{gather}
If we take the output of our equation to be position(q) then we get:
\begin{gather}
	y = \begin{bmatrix} 1 & 0\end{bmatrix}
	\begin{bmatrix} q \\ \frac{dq}{dt}\end{bmatrix} 
	+0F
\end{gather}
In this case we will then write out state-space equation in the form: 
\begin{gather}
	\frac{dx_c}{dt}= Ax_c+Bu\\
	y = Cx_c + Du
\end{gather}   
where the solution is:
\begin{gather}
	x(t) =e^{At}x_c(t)+ \int_{0}^t e^{A(t-\tau)}Bu(\tau)d\tau
\end{gather}                                               
We now quickly consider what $e^{At}$ is in matrix form:
\begin{gather}
	e^{At}:=\mathbb{I}+At + \frac{A^2}{2}t^2+...+\frac{A^k}{k}t^{k}\\
	e^{At} = Te^{\Lambda t}T^{-1}
\end{gather}                                                                         
Now we have to try an consider this in a discrete time state-space model so we want $x_cb \rightarrow x_d$.\\
Now we take $x_d(k)=x_c(kT)$.\\
I'm going to attempt this proof but first write down the starting point/s and the answer as these are the things we actually need to know whereas the proof is not important but may be useful to understand.\\
\begin{gather}
	x((k+1)T)= e^{A(k+1)T}x(0) + \int_0^{(k+1)T}e^{A(k+1)T-\tau}Bu(\tau)d\tau	\\
	x(kT)= e^{AkT}x(0) + \int_0^{kT}e^{kT-\tau}Bu(\tau)d\tau	
\end{gather}
``From this conclude'':
\begin{equation}
	x((k+1)T)= e^{AT}x(kT)+ e^{A(k+1)T}\int^{(k+1)T}_{kT}e^{A(-\tau)}
\end{equation}

After some mathematical wizardry which was taught three times that day (and I may or may not include in these notes) we arrive at the final answer:

\begin{equation}
	x((k+1)T)=e^{AT}x(kT)+\int^{T}_{0}e^{A(T-\tau)}Bd\tau u(kT)
\end{equation}
for this to work the input MUST be a zero order hold block (ZOH) as this allows the association made in the previous equation which allowed $u(\tau)$ to be equal to $u(kT)$.\\ 
You should now recognise this equation as it is in the form of the original state-space equation (it also might be worth noting the similarity to the solution for the continuous time example from earlier). For clarification here is it written differently:
\begin{gather}
	x((k+1)T) = G x(kT) + H u(kT)\\
	G:=e^{AT} \qquad H:=\int^{T}_{0} e^{A(T-\tau)}Bd\tau
\end{gather}
Note that G and H are functions of A and B.

\subsection{The rest}
 The rest of this lecture just covered an example of state space equations similar to the one given in the self study lecture so I will refrain from repeating myself. If I can remember it then I will make a note about singular matrices which he talked about in the extra lecture.                                                                                                                             
\section{Stability and Controllability}
\subsection{Recap}
To kick off this lecture he gave a quick recap with a useful table, so here it is.
\begin{table}[htp!]
\centering
\begin{tabular}{l|l}
Continuous Time & Discrete Time \\ \hline 
A               & $e^{AT}$ \\
B               & $\int^{T}_{0}e^{A(T-\tau)}d\tau$ \\
C               & C \\
D               & D
\end{tabular}
\end{table}
\subsection{Property preservation}
	This section concerns the property preservation under discretion (ZOH).\\
	\textbf{Asymptotic Stability} of a state space equation means that "solutions of $\frac{dx}{dt}=Ax$ go to 0 as $t \rightarrow \infty$". This can also be written as "all eigenvalues of A are in the open left hand plane".\\
	An example of this is:
	\begin{equation}
		\frac{dx}{dt}= A x \qquad x(t) = e^{A T}x(0)
	\end{equation}
	when interpreted with spectral decomposition:
	\begin{gather}
		A=V\Lambda V^{-1} \qquad \frac{dx}{dt}= A x = V\Lambda V^{-1}x\\
		V^{-1}\frac{dx}{dt} = \Lambda V^{-1}x \qquad \frac{d(V^{-1})x}{dt} = \Lambda (V^{-1}x) \qquad x' := V^{-1}x
	\end{gather}

	So what does it mean for $x(k+1) = A_d x(k)$ to be asymptotically stable ?\\
	$x(k)\rightarrow 0$ as $k\rightarrow \infty$ $\Longleftrightarrow$ all eigenvalues of $A_d$ lie in the open unit disk.\\\\
	We will consider this using an example with respect to the previous statement: \\
	\begin{gather}
	x(k+1)=\alpha x(k)\\
	x(k) = \alpha^{k} x(0)
	\end{gather}
	You can see that x(k) will tend to 0 if and only if $\alpha \leq 1$\\
	Note that the eigenvalues of $e^{AT}$ are $e^{\lambda i T}$\\  
	eg. $\lambda_i = r_i+j s_i$ where $r_i < 0$
	\begin{gather}
		|e^{\lambda_i}T|= |e^{r_iT+js_iT}| = |e^{r_iT}e^{js_iT}| = |e^{r_iT}||e^{js_iT}|\\
		|e^{r_iT}| < 1
	\end{gather}
	This system is stable as all $\lambda_i's$ are stable. s.t. $Re(\lambda_i)< 0$ for all of i. Hence we ignore the imaginary part in the previous equation.
	\subsection{Controllability} 
	Can I reach an arbitrary $x_{final}$ starting from any arbitrary $x_{initial}$   \\
	This is also equivalent with: "Given arbitrary $X_{in},X_{f}$ does there exist a non-negative integer M and an input sequence $u(0) ... u(M-1)$ such that$ X(M)=x_{final}$ when $x(0) = x_{initial}$"\\
	This will be considered with this case:\\
	\begin{gather}
	 x(0) = 0 \qquad x(M) = x_{final} = \sum^{M-1}_{j = 0}A^{m-1-j}Bu(j)\\
	 = 
	  		\begin{bmatrix} 
 			B \\ AB \\ ...\\ A^{M-1}B
 		\end{bmatrix}
 		 		 \begin{bmatrix} 
 			u(m-1)&u(m-2)&...&u(0)
 		\end{bmatrix}\\
 		:= \mathcal{C}_m  U 
	\end{gather}
	$x_{final}$ is reachable from $x(0)  = 0$ if and only if there exists an M and u(.) such that the previous equation holds.\\
	\begin{gather}
		\begin{bmatrix} 
	 		x_{f1}\\x_{f2}\\...\\x_{fm}
	 	\end{bmatrix}	
	 	= \mathcal{C}_m
	 	\begin{bmatrix} 
 			u(m-1)\\u(m-2)\\...\\u(0)
 		\end{bmatrix}
 		= \mathcal{C}_m \bar{U}_m
	\end{gather}
	A solution to $\bar{U}_m$ exists is and only if the columns of $\mathcal{C}_m$ the whole space $\mathcal{R}^M$. $\mathcal{C}_m$ contains n linearly independent columns. \\
	A quick example explaining the previous comment:\\
	\begin{gather}
		\begin{bmatrix}
			1\\1 
		\end{bmatrix}=
		\begin{bmatrix}
			1 & 3\\
			0 & 0 
		\end{bmatrix}
		\begin{bmatrix}
			u(1)\\u(0) 
		\end{bmatrix}
	\end{gather}
	the left hand side cannot be expressed as a linear combination of columns of the design matrix.\\
	For a single input system, reachability exists if and only if $\mathcal{C}_m$ has n linearly independent columns.  
	
	\section{Controllability (Continued)}
		 This first part of this lecture was a little recap by the looks of it so I will skip over that.\\
		% Can I, starting from an arbitrary $x(0)$ achieve an arbitraty $x_final$ in a finite amount of time ? if so then the system is controlable.
		% \begin{gather}
		% 	x(k) = A^{k}x(0) + \sum^{k-1}_{j=0}A^{k-1-j}Bu(j)\\
		% 	x(k) - A^{k}x(0) = 
		% 	\begin{bmatrix}
		% 		B & AB & ... & A^{k-1}B
		% 	\end{bmatrix}
		% 	\begin{bmatrix}
		% 		u(k-1) \\...\\u(1) \\ u(0)
		% 	\end{bmatrix}
		% \end{gather}
		% Controlablility can be found if and only if the columns of  
		We start this lecture with an example of a transfer function and determining if it a stable system.\\ 
		With starting parameters A, B and C of:
		\begin{gather}
			A= 
			\begin{bmatrix}
				0 & 1 \\
				-2 & -3 
			\end{bmatrix}
			\qquad 
			B = 
			\begin{bmatrix}
				0 \\ 1 
			\end{bmatrix}
			\qquad 
			C = 	
			\begin{bmatrix}
				1 & 0 
			\end{bmatrix}
			\qquad			
			\mathcal{C}(AB) = 
			\begin{bmatrix}
				0 & 1 \\
				1 & -3 
			\end{bmatrix}
		\end{gather}
		As the columns of $\mathcal{C}(AB)$ are all linearly independent this shows that this system is controllable.\\
		The transfer function of this system is given as (not entirely sure why I need to check the slides on this one) and simplified to:
		\begin{gather}
			H(z)=C(z\mathbb{I}-A )^{-1}B = 
			\begin{bmatrix}
				1 & 0 
			\end{bmatrix}
			\begin{bmatrix}
				z & -1 \\
				2 & z+3 
			\end{bmatrix}^{-1}
			\begin{bmatrix}
				0 \\ 1 
			\end{bmatrix}\\
			=
			\begin{bmatrix}
				1 & 0 
			\end{bmatrix}
			\frac{1}{z^2+3z+2}
			\begin{bmatrix}
				z+3 & 1 \\
				-2 & z 
			\end{bmatrix}^{-1}
			\begin{bmatrix}
				0 \\ 1 
			\end{bmatrix}\\
			= \frac{1}{z^2+3z+2} = \frac{1}{(z-1)(z+2)}
		\end{gather}
		We can tell that this current system is not stable by the fact that the roots of the transfer function do not lie within the unit circle. The next question is, can we now make this system stable by adding negative feedback ?


	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.1]{images/l10_negativefeedback}
	\end{figure}
	\FloatBarrier
	We need to find the transfer function between v and y (closed loop transfer function).
	\begin{gather}
		y(z) = \frac{1}{z^2+3z+2}u(z)
		\qquad
		u(z) = v(z)-\alpha y(z)\\
		y(z) = \frac{1}{z^2+3z+2}(v(z)-\alpha y(z))\\
		y(z) = \frac{\frac{1}{z^2+3z+2}}{1+\frac{\alpha}{z^2+3z+2}}v(z) = \frac{1}{z^2+3z+(2+\alpha)}
	\end{gather}
	We then consider for $\alpha > 0 $ then find the roots by taking the modulus of the quadratic equation. It is clear that this does not work therefore cannot be stable. The result is that $4\alpha <-4$ which cannot be true if $\alpha$ is positive.\\\\

	We will now consider this following set of equations (same as above):


	\begin{gather}
		x(k+1) = 
		\begin{bmatrix}
			0 & 1 \\
			-2 & -3
		\end{bmatrix}
		 x(k)+ 
		\begin{bmatrix}
			0\\1
		\end{bmatrix}
		u(k) \\
		y(k) = 
		\begin{bmatrix}
			1 & 0
		\end{bmatrix}
		x(k)
	\end{gather}
	\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.15]{images/l10_negativefeedback2}
	\end{figure}
	\FloatBarrier
	from this we can see that:
	\begin{equation}
		u(k) = -kx(k)+v(k) \qquad  k\in \mathbb{R}^{m\times n} (\mathbb{R}^{1\times 2})
	\end{equation}
	We will first consider the part of the system in the dashed and dotted line which from now we will call (1):

	\begin{gather}
		x(k+1) = 
		\begin{bmatrix}
			0 & 1 \\
			-2 & -3
		\end{bmatrix}
		 x(k)+ 
		\begin{bmatrix}
			0\\1
		\end{bmatrix}
		(-
		\begin{bmatrix}
			k_0 &k_1
		\end{bmatrix}
		x(k) + v(k)
		) \\
		=[		\begin{bmatrix}
			0 & 1 \\
			-2 & -3
		\end{bmatrix}
		+
		\begin{bmatrix}
			0 & 0 \\
			-k_0 & -k_1
		\end{bmatrix}
		]+
		\begin{bmatrix}
			0\\1
		\end{bmatrix}
		u(k) \\
		=
		\begin{bmatrix}
			0 & 1 \\
			-2-k_0 & -3-k_1
		\end{bmatrix}
		+
		\begin{bmatrix}
			0\\1
		\end{bmatrix}
		u(k)
	\end{gather}
	now to consider y(k):

	\begin{gather}
		y(k)=
		\begin{bmatrix}
			1 & 0 
		\end{bmatrix}
		[z\mathbb{I}-
		\begin{bmatrix}
		0&1\\
		-2-k_0& -3-k_1
		\end{bmatrix}
		]^{-1}
		\begin{bmatrix}
			0\\1
		\end{bmatrix}
	\end{gather}

	this is our new transfer function.


	\begin{gather}
		=
		\begin{bmatrix}
			1 & 0 
		\end{bmatrix}
		\frac{1}{z^2+(3+k_1)z+(2+k_0)}
		\begin{bmatrix}
		z+3+k_1&1\\
		-(2+k_0)  & z
		\end{bmatrix}
		\begin{bmatrix}
			0\\1
		\end{bmatrix}\\
		= \frac{1}{z^2+(3+k_1)z+(2+k_0)}
	\end{gather}
	If we are given our `desired' closed loop poles as $-\frac{1}{2}$  \&  $-\frac{1}{3}$ then we know that the denominator should be equal to:
	\begin{equation}
	 (z+\frac{1}{2})(z+\frac{1}{3}) = z^2+\frac{5}{6}z+\frac{1}{6} = z^2+(3+k_1)z+(2+k_0)
	 \end{equation}
	 from this we can work out $k_0 = -\frac{11}{6}$ and $k_1 = -\frac{13}{6}$ \\

	Ex \#2:
	\begin{gather}
			A= 
			\begin{bmatrix}
				2 & 0 \\
				0 & 1 
			\end{bmatrix}
			\qquad 
			B = 
			\begin{bmatrix}
				1 \\ 1 
			\end{bmatrix}
			\qquad 
			C = 	
			\begin{bmatrix}
				1 & 1
			\end{bmatrix}\\
			TF = 			
			\begin{bmatrix}
				1 & 1
			\end{bmatrix}
			\begin{bmatrix}
				z-2 & 0 \\
				0 & z-1 
			\end{bmatrix}^{-1}
			\begin{bmatrix}
				1 \\ 1 
			\end{bmatrix}
			= \frac{1}{z-2}+\frac{1}{z-1}=\frac{2z-3}{(z-2)(z-1)}
			% \qquad			
			% \mathcal{C}(AB) = 
			% \begin{bmatrix}
			% 	0 & 1 \\
			% 	1 & -3 
			% \end{bmatrix}
		\end{gather}	 
		Following the idea of state feedback:\\
		We define $k=(k_0\quad k_1)$
		\begin{gather}
			A-Bk = 
			\begin{bmatrix}
				2& 0\\
				0&1
			\end{bmatrix}-
			\begin{bmatrix}
				k_0& k_1\\
				k_0& k_1
			\end{bmatrix}=
			\begin{bmatrix}
				2-k_0 & -k_1\\
				-k_0 & 1-k_1
			\end{bmatrix}
		\end{gather}

		we then find:
		\begin{gather}
			[z\mathbb{I}-(A-Bk)]^{-1}=\frac{1}{(z-k_0-2)(z+k_1-1)-k_0k_1}
			\begin{bmatrix}
				z+k_1-1 & -k_1\\
				-k_0 & z+k_0-2
			\end{bmatrix}
		\end{gather}
		we only care about the denominator of that equation and we want this to be true:
		\begin{gather}
			z^2+z(k_1-3+k_0)+(-k_0-2k_2+2)= z^2+\frac{5}{6} z +\frac{1}{6}
		\end{gather}
		From this we can then calculate $k_0$ and $k_1$:
		\begin{gather}
			\frac{5}{6}=k_1-3+k_0\\
			\frac{1}{6}=2-k_0+2k_1\\
			\begin{bmatrix}
				1 &1 \\
				-1 &2 
			\end{bmatrix} 
			\begin{bmatrix}
				 k_0 \\ k_1
			\end{bmatrix}
			= 
			\begin{bmatrix}
				\frac{23}{6}\\
				\frac{13}{3}
			\end{bmatrix} 
		\end{gather}
		This is actually enough proof that there is a solution for k.\\
		\subsection{Theory: kalman}
		Given:
		\begin{equation}
			x(k+1) = A x(k) + Bu(k)
		\end{equation}
		\begin{equation}
			y(k) = C x(k) + Du(k)
		\end{equation}
		Assume A, B are uncontrollable:\\
		let $P_i \in \mathbb C$ (complex) be arbitrary\\
		There exists a $k \in \mathbb R^{m\times n}$ such that $C(z\mathbb I - (A-Bk))^{-1}B$ has the $P_i$'s as poles.\\
		``Arbitrary Pole assignment via controllability \& state feedback''
		\section{Observability}
		This lecture has an overview of the previous lecture then goes into detail about observability before diving into more detail.\\
		Given data ${U_k}$ and ${y_k}$ and the static equations (ie. given A,B,C,D) determine x(0).\\
			\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.4]{images/l11_1}
		\end{figure}
		The system above can be described by the equations:
		% \begin{gather}
		% 	x(k)=A^kx(0)+\sum^{k-1}_{j=0}A^{k-1-j}Bu(j)\\
		% 	y(k)=CA^kx(0)+\sum^{k-1}_{j=0}CA^{k-1-j}Bu(j)+Du(j)\\
		% 	y(0)=Cx(0)+Du(0)\\
		% 	y(1)=CAx(0)+CBu(0)+Du(1)\\
		% 	y(1)=CA^2x(0)+CABu(0)+...

		% \end{gather}

		\begin{gather}
			x(k)=A^kx(0)+\sum^{k-1}_{j=0}A^{k-1-j}Bu(j)\\	
						y(k)=CA^kx(0)+\sum^{k-1}_{j=0}CA^{k-1-j}Bu(j)+Du(j)\\
					y(0)=Cx(0)+Du(0)\\
					y(1)=CAx(0)+CBu(0)+Du(1)\\
					y(2)=CA^2x(0)+CABu(0)+...
		\end{gather}
		The problem equivalently stated is: can I determine x(0) from the equations:
		 \begin{gather}
					y(0)-Du(0) =Cx(0)\\
					y(1)-Du(1)-CAx(0)=CBu(0)\\
					y(N)-Du(1)-... =CA^{N-1}x(0)
		\end{gather}
		This can be written in the form: 
		\begin{gather}
			y=
			\begin{bmatrix}
				C\\
				CA\\
				\vdots\\
				CA^{N-1}
			\end{bmatrix}x(0)
		\end{gather}
		set N linear equations in the n unknowns $x_i(0)$ $i=1,\cdots,n$\\
		we can uniquely identify $x(0)$ if and only if there are n linearly dependant equations  $\Leftrightarrow$ n linear dependant.\\

		\subsection{Observers}
		A system that estimates $x(k)$ on the basis of knowledge of ${u_k}$ and ${y_k}$.\\
		"Estimates" means: the observer computes a vector $x'(k)$ such that $\lim\limits_{k\rightarrow \infty}x(k)-x'(k)=0$.\\
		\begin{figure}[htp]
		\centering
		\includegraphics[scale=0.4]{images/l11_2}
		\end{figure}
		We want to be have access to x(k) so that we can have a state control feedback system but we don't necessarily have access to it. Therefore we need to 'observe' the system and work out what x'(k) is.\\
		
		We must then create the observer:\\
		A':A-LC \qquad B':=B \qquad	C':=C \qquad	D':=D \\

		Kalman states that id (A,C) observable then:
		\begin{gather}
			\begin{bmatrix}
				C\\
				CA\\
				\vdots\\
				CA^{N-1}
			\end{bmatrix}
		\end{gather}
		has full column rank N.\\
		Then there exists a $L\in\mathbb{R}^{n\times p}$ such that $A-LC$ has all its eigenvalues in the open unit disk. AND $x(k)-x'(k)=(A-LC)^k(x(0)-x'(0))$\\
		We will consider the first sentence first:\\ 
		We already know the controllability of x(k) (A,B) can be found by finding the full rank of $			\begin{bmatrix}
				C&
				CA&
				\cdots&
				CA^{N-1}
			\end{bmatrix}$\\
		we find the observability of y(k) (A,C) can be found by finding the column rank of 			$\begin{bmatrix}
						C\\
						CA\\
						\vdots\\
						CA^{N-1}
					\end{bmatrix}\\
								\begin{bmatrix}
				C^T&
				A^TC^T&
				\cdots&
				(A^{N-1})^TC^T
			\end{bmatrix}$ has full row rank n!\\
			(A,C) is controllable if and only if $[A^T,C^T]$ is controllable, therefore if and only if every choice $\lambda_1...\lambda_n$ there exists an $L^T=k$ such that the eigenvalues of $A^T-C^TL^T$ are $\lambda_1...\lambda_n$.\\
			\subsection{Nilpotent Matrices}
			This matrix has a nilpotent order of 1:\\
			\begin{gather}
				J=
				\begin{bmatrix}
					0 & 0 \\
					0 & 0 \\
				\end{bmatrix}
			\end{gather}
			This matrix has a nilpotent order of 2:\\
			\begin{gather}
				J=
				\begin{bmatrix}
					0 & 1 \\
					0 & 0 \\
				\end{bmatrix}\\
								J^2=
				\begin{bmatrix}
					0 & 0 \\
					0 & 0 \\
				\end{bmatrix}
			\end{gather}
			I think you get the idea!
\section{Observers Continued}
	\subsection{Recap}
	I think he was a bit more clear when going over this topic for the second time.\\
	if (A,C) is observable, $A'=A-LC$ for a suitable L.\\
	the following equation shows the observer:
	\begin{gather}
		y'(k+1)=Cx'(k)\\
		x'(k+1) =Ax'(k) + Bu(k) + L(y'(k)-y(k))\\
		x'(k+1) =Ax'(k) + Bu(k) + L(Cx'(k)-Cx(k))\\
		e(k+1) = x'(k+1) - x(k+1) = A(x'(k)-x(k))+LC(x'(k)-x(k))\\
		e(k+1) = (A+LC)e(k)
	\end{gather}	
	from this we then can derive some rules:\\
	$x'(k)\rightarrow_{k\rightarrow 0}x(k)$ if and only if $x'(k)-x(k)\rightarrow_{k\rightarrow 0}0$ if and only if (A-LC) has all of its eigenvalues in the open unit disk.\\

	Example:
	\begin{gather}
		A = 
		\begin{bmatrix}
			1 & 2\\
			0 & 3 
		\end{bmatrix}
		\qquad
		B = 
		\begin{bmatrix}
			4 \\
			5
		\end{bmatrix}
		\qquad 
		C = 
		\begin{bmatrix}
		 	6 & 7 
		\end{bmatrix}
	\end{gather}
	is the pair (A,C) observable ?\\
	\begin{gather}
		\begin{bmatrix}
			C\\
			CA\\
			\vdots\\
			CA^{N-1}
		\end{bmatrix}\rightarrow_{N=2}
		\begin{bmatrix}
		C\\CA
		\end{bmatrix}=
		\begin{bmatrix}
			6 & 7\\
			6 & 3
		\end{bmatrix}		
	\end{gather}
	The (C,A) is non singular, hence the system is observable. By careful choice of $L\in \mathbb{R}^{2x1}$ I can place the eignvalues of $A-LC$ in the open unit disk.\\

	We choose all of the eigenvalues of $A-LC$ at $\lambda = 0$\\

	\begin{gather}
		A-LC = 
		\begin{bmatrix}
			1 & 0\\
			0 & 3 
		\end{bmatrix}-
		\begin{bmatrix}
			L_0\\
			L-1
		\end{bmatrix}
		\begin{bmatrix}
			6 & 7 
		\end{bmatrix}=
		\begin{bmatrix}
			1-6L_0& 2-7L_0\\
			-6L-1 & 3-7L_1
		\end{bmatrix}
	\end{gather}
	We can find the characteristic polynomial using $det(z\mathbb{I}-(A-LC))$ is\\ $(z-1+6L_0)(s-3+7L_1)+6L_1(2-7L_0)$\\
	$=z^2+3z+7L_1z-z-7L_1+6L_0z-18L_0+\cancel{42L_0L_1}+12L_1-\cancel{42L_0L_1}$\\
	$=z^2+z(-4+7L_1+6L_0)+(3+5L_1-18L_0)$ \\
	We want to chose an L which removes the $z^1$ and the $z^0$ terms from the above equation. We want $\lambda$ to be zero.\\
	\begin{gather}
		-4+7L_1+6L_0 = 0\\
		3+5L_1-18L_0 = 0\\
		\downarrow\\
		\begin{bmatrix}
			6 & 7\\
			-18 & 5 
		\end{bmatrix}
		\begin{bmatrix}
			L_0\\
			L_1
		\end{bmatrix}=
		\begin{bmatrix}
			4\\
			-3
		\end{bmatrix}
	\end{gather}
	If the left hand side of the above equation is non singular then L exists.


	What is so special about choosing $L=0$ ? \\
	We will consider the two equations:
	\begin{gather}
		e(k+1) = (A-LC)e(k)\\
		e(k) = (A-LC)^k e(0)
	\end{gather}
	if a matrix M has all of its eigenvalues = 0 then there exists a $k \leq n $ such that $M^k = 0$. This is the idea of nilpotent matrices from the previous lecture.

	\subsection{Deadbeat Observer}
		If $(A - LC)$ has all of its eigenvalues = 0, at most of n steps $e(k)=(A-LC)^ke(0) = 0$ $e(0)=0$. This suggests that there will be a perfect reconstruction after n steps.\\
		This shows that the deadbeat controller will make its estimation quickly. I f there is noise in the system then this could make an inaccurate controller.

	\subsection{Feedback}
		% We will now refer to the input as v(k) 
		what is v(k) here?
		\begin{gather}
			x(k+1)=Ax(k) + Bu(k)\\
			x(k+1)=Ax(k) + B(-kx'(k)+v(k))\\
			x'(k+1)=Ax'(k) + Bu(k)+L(Cx'(k)-Cx(k))\\
			x'(k+1)=Ax'(k) + B(-kx'(k)+v(k))+LC(x'(k)-x(k))\\
			\begin{bmatrix}
				x(k+1)\\
				x'(k+1)
			\end{bmatrix}=
			\begin{bmatrix}
				&&\\
				&&
			\end{bmatrix}
			\begin{bmatrix}
				x(k)\\
				x'(k)
			\end{bmatrix}
			+
			\begin{bmatrix}
				&\\
				&
			\end{bmatrix} v(k)
		\end{gather}
		The state for the observer controller scheme is:
		\begin{gather}
						\begin{bmatrix}
				x(k)\\
				x'(k)
			\end{bmatrix}
		\end{gather}
		the state could also be in terms of the error:
		\begin{gather}
						\begin{bmatrix}
				x(k)\\
				e(k)
			\end{bmatrix}
		\end{gather}
		We can also write this as:
		\begin{gather}
			\begin{bmatrix}
				1 & 0\\
				1 & 1
			\end{bmatrix}
			\begin{bmatrix}
				x(k)\\
				e(k)
			\end{bmatrix}
			= 
			\begin{bmatrix}
				x(k)\\
				x'(k)
			\end{bmatrix}
		\end{gather}
		Separation principle: one can design the observer and controller separately.  

		\begin{gather}
			\begin{bmatrix}
				x(k+1)\\
				e(k+1)
			\end{bmatrix}=
			\begin{bmatrix}
				A-Bk & *\\
				0 &  A-LC
			\end{bmatrix}
			\begin{bmatrix}
				x(k)\\
				e(k)
			\end{bmatrix}
			+
			\begin{bmatrix}
				B\\
				0
			\end{bmatrix} v(k)
		\end{gather}

		\subsection{Realization}
			This is the part of the lecture which I boiled alive. I need to go over this to understand it.
\end{document}
